{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85120305",
   "metadata": {},
   "source": [
    "ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì™€ì„œ ê²°ê³¼ë¥¼ ì‹œê°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54eddf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875b6f5",
   "metadata": {},
   "source": [
    "ë³€ìˆ˜ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3171ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== VISUALIZATION ì„¤ì • =====\n",
    "VISUALIZE_ALL = True  # True: ì „ì²´ í…ŒìŠ¤íŠ¸ì…‹, False: ì¼ë¶€ë§Œ\n",
    "NUM_SAMPLES = 20  # VISUALIZE_ALL=Falseì¼ ë•Œ ì‹œê°í™”í•  ìƒ˜í”Œ ìˆ˜\n",
    "RANDOM_SAMPLE = True  # True: ëœë¤ ìƒ˜í”Œë§, False: ìˆœì°¨ì \n",
    "\n",
    "# ===== ì‹¤í—˜ ì„¤ì • =====\n",
    "DATASET_ROOT = \"./dataset\"\n",
    "TEST_DATASET = 'WHU-CD'\n",
    "TEST_MODEL = 'Changer'  # ëª¨ë¸ëª…\n",
    "TEST_CLASS = 'ChangerSkeleton'  # í´ë˜ìŠ¤ëª…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b53ce",
   "metadata": {},
   "source": [
    "ê²½ë¡œ ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86427faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment directory: experiments/WHU-CD/Changer/ChangerSkeleton\n",
      "Checkpoint path: experiments/WHU-CD/Changer/ChangerSkeleton/checkpoints/best_model.pth\n"
     ]
    }
   ],
   "source": [
    "# ëª¨ë¸ íŒŒì¼ëª… ë³€í™˜\n",
    "model_file = TEST_MODEL.replace('-', '').lower()\n",
    "model_class = TEST_CLASS\n",
    "\n",
    "# í´ë˜ìŠ¤ëª…ì´ ëª¨ë¸ëª…ê³¼ ê°™ìœ¼ë©´ ëª¨ë¸ëª…ë§Œ ì‚¬ìš©, ë‹¤ë¥´ë©´ í´ë˜ìŠ¤ëª… ì‚¬ìš©\n",
    "if model_class.lower() == TEST_MODEL.lower():\n",
    "    class_folder = TEST_MODEL\n",
    "else:\n",
    "    class_folder = model_class\n",
    "\n",
    "# ê²½ë¡œ ì„¤ì •: experiments/ë°ì´í„°ì…‹/ëª¨ë¸/í´ë˜ìŠ¤/\n",
    "exp_dir = Path(f\"experiments/{TEST_DATASET}/{TEST_MODEL}/{class_folder}\")\n",
    "checkpoint_path = exp_dir / \"checkpoints\" / \"best_model.pth\"\n",
    "\n",
    "print(f\"Experiment directory: {exp_dir}\")\n",
    "print(f\"Checkpoint path: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad5927",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80e229f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: 1\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ì¼ GPU ì‚¬ìš©\n",
    "GPU_ID = 1\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = str(GPU_ID)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using GPU: {GPU_ID}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# ë©€í‹° GPU ì‚¬ìš© \n",
    "# # GPU_IDS = [0, 1, 2, 3]  # ì‚¬ìš©í•  GPU ë¦¬ìŠ¤íŠ¸\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, GPU_IDS))\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# USE_MULTI_GPU = len(GPU_IDS) > 1 and torch.cuda.device_count() > 1\n",
    "# print(f\"Using GPUs: {GPU_IDS}\")\n",
    "# print(f\"Available GPU count: {torch.cuda.device_count()}\")\n",
    "# if USE_MULTI_GPU:\n",
    "#     BATCH_SIZE = BATCH_SIZE * len(GPU_IDS)  # ë©€í‹° GPUì‹œ ë°°ì¹˜ í¬ê¸° ì¡°ì •\n",
    "#     print(f\"Adjusted batch size for multi-GPU: {BATCH_SIZE}\")\n",
    "# ì‹œë“œ ì„¤ì • (ì¬í˜„ê°€ëŠ¥ì„±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4872564e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully loaded: ChangerSkeleton from models.changer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/userHome/userhome4/kyoungmin/miniconda3/envs/cd_efficient/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/userHome/userhome4/kyoungmin/miniconda3/envs/cd_efficient/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: ChangerSkeleton\n",
      "Best F1: 0.7542\n",
      "Loaded 4233 images from WHU-CD/train\n",
      "Loaded 747 images from WHU-CD/val\n",
      "Loaded 2700 images from WHU-CD/test\n",
      "Test samples: 2700\n"
     ]
    }
   ],
   "source": [
    "# %% ëª¨ë¸ ë¡œë“œ\n",
    "import importlib\n",
    "\n",
    "def get_model_class(module_name, class_name):\n",
    "    \"\"\"ëª¨ë¸ ë™ì  import\"\"\"\n",
    "    module_path = f'models.{module_name}'\n",
    "    \n",
    "    try:\n",
    "        module = importlib.import_module(module_path)\n",
    "        model_class = getattr(module, class_name)\n",
    "        print(f\"âœ… Successfully loaded: {class_name} from {module_path}\")\n",
    "        return model_class\n",
    "    except ImportError as e:\n",
    "        print(f\"âŒ Cannot import module {module_path}\")\n",
    "        raise ImportError(f\"Module import failed: {e}\")\n",
    "    except AttributeError as e:\n",
    "        print(f\"âŒ Class {class_name} not found in {module_path}\")\n",
    "        raise AttributeError(f\"Class not found: {e}\")\n",
    "\n",
    "# ëª¨ë¸ ë¡œë“œ\n",
    "ModelClass = get_model_class(model_file, model_class)\n",
    "model = ModelClass(num_classes=1).to(DEVICE)\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model: {ModelClass.__name__}\")\n",
    "print(f\"Best F1: {checkpoint.get('best_f1', 0):.4f}\")\n",
    "\n",
    "# %% ë°ì´í„°ë¡œë”\n",
    "from utils import create_dataloaders\n",
    "\n",
    "_, _, test_loader = create_dataloaders(\n",
    "    root_dir=DATASET_ROOT,\n",
    "    dataset_name=TEST_DATASET,\n",
    "    batch_size=1,\n",
    "    num_workers=2,\n",
    "    augment=False\n",
    ")\n",
    "\n",
    "total_samples = len(test_loader)\n",
    "print(f\"Test samples: {total_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51566f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %% ì‹œê°í™” ì‹¤í–‰\n",
    "# from utils.visualization import save_visualization\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(f\"Starting Visualization\")\n",
    "# print(f\"Dataset: {TEST_DATASET}\")\n",
    "# print(f\"Model: {TEST_MODEL}\")\n",
    "# print(f\"Total samples: {total_samples}\")\n",
    "# print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# # ìƒ˜í”Œ ì„ íƒ\n",
    "# if VISUALIZE_ALL:\n",
    "#     sample_indices = list(range(total_samples))\n",
    "#     print(f\"ğŸ“Š Visualizing ALL {total_samples} samples\")\n",
    "# else:\n",
    "#     if RANDOM_SAMPLE:\n",
    "#         sample_indices = sorted(random.sample(range(total_samples), min(NUM_SAMPLES, total_samples)))\n",
    "#         print(f\"ğŸ“Š Visualizing {len(sample_indices)} RANDOM samples\")\n",
    "#     else:\n",
    "#         sample_indices = list(range(min(NUM_SAMPLES, total_samples)))\n",
    "#         print(f\"ğŸ“Š Visualizing first {len(sample_indices)} samples\")\n",
    "\n",
    "# print(f\"Sample indices: {sample_indices[:10]}...\" if len(sample_indices) > 10 else f\"Sample indices: {sample_indices}\")\n",
    "# print()\n",
    "\n",
    "# # ì‹œê°í™” ì‹¤í–‰\n",
    "# with torch.no_grad():\n",
    "#     for idx, batch in enumerate(tqdm(test_loader, desc=\"Visualizing\")):\n",
    "#         # ì„ íƒëœ ìƒ˜í”Œë§Œ ì²˜ë¦¬\n",
    "#         if idx not in sample_indices:\n",
    "#             continue\n",
    "        \n",
    "#         try:\n",
    "#             # ë°°ì¹˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "#             pre_img = batch['img1'].to(DEVICE)\n",
    "#             post_img = batch['img2'].to(DEVICE)\n",
    "#             gt_mask = batch['label'].to(DEVICE)\n",
    "#             filename = batch['filename']\n",
    "            \n",
    "#             # íŒŒì¼ëª… ì²˜ë¦¬ (ë°°ì¹˜ í¬ê¸°ê°€ 1ì´ë¯€ë¡œ)\n",
    "#             if isinstance(filename, (list, tuple)):\n",
    "#                 file_name = filename[0]\n",
    "#             else:\n",
    "#                 file_name = filename\n",
    "            \n",
    "#             # ì˜ˆì¸¡\n",
    "#             pred_mask = model(pre_img, post_img)\n",
    "            \n",
    "#             # Sigmoid ì ìš© (í•„ìš”í•œ ê²½ìš°)\n",
    "#             if pred_mask.max() > 1.0 or pred_mask.min() < 0:\n",
    "#                 pred_mask = torch.sigmoid(pred_mask)\n",
    "            \n",
    "#             # í™•ì¥ì ì œê±°\n",
    "#             file_name = Path(file_name).stem\n",
    "            \n",
    "#             # ì‹œê°í™” ì €ì¥\n",
    "#             save_visualization(\n",
    "#                 pre_img=pre_img[0],\n",
    "#                 post_img=post_img[0],\n",
    "#                 gt_mask=gt_mask[0],\n",
    "#                 pred_mask=pred_mask[0],\n",
    "#                 save_path=str(save_dir),\n",
    "#                 file_name=file_name\n",
    "#             )\n",
    "            \n",
    "#         except Exception as e:\n",
    "#             print(f\"\\nâš ï¸ Error processing sample {idx}: {e}\")\n",
    "#             import traceback\n",
    "#             traceback.print_exc()\n",
    "#             continue\n",
    "\n",
    "# print(\"\\n\" + \"=\"*50)\n",
    "# print(f\"âœ… Visualization completed!\")\n",
    "# print(f\"ğŸ“ Saved to: {save_dir}\")\n",
    "# print(f\"ğŸ“Š Total visualized: {len(sample_indices)} samples\")\n",
    "# print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26254a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "F1-Score Based Visualization (Samples with Errors)\n",
      "Dataset: WHU-CD\n",
      "Model: Changer\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% F1 Score ê¸°ë°˜ ì‹œê°í™” (ì‹¤ì œ ì˜¤ë‹µì´ ìˆëŠ” ì¼€ì´ìŠ¤ë§Œ)\n",
    "from utils.visualization import save_visualization\n",
    "import json\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"F1-Score Based Visualization (Samples with Errors)\")\n",
    "print(f\"Dataset: {TEST_DATASET}\")\n",
    "print(f\"Model: {TEST_MODEL}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# ===== ì„¤ì • =====\n",
    "EXCLUDE_PERFECT = True  # True: F1=1.0ì¸ ìƒ˜í”Œ ì œì™¸\n",
    "EXCLUDE_NO_CHANGE = True  # True: FP=0, FN=0ì¸ ìƒ˜í”Œ ì œì™¸ (ë³€í™” ì—†ëŠ” ì •ë‹µ ì¼€ì´ìŠ¤)\n",
    "NUM_WORST_SAMPLES = 400  # í•˜ìœ„ Nê°œ\n",
    "NUM_BEST_SAMPLES = 400   # ìƒìœ„ Nê°œ\n",
    "# ================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113331b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Calculating F1 scores for all samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating F1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2700/2700 [00:21<00:00, 126.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… F1 scores calculated for 2700 samples\n",
      "\n",
      "ğŸ” Perfect samples (F1 â‰ˆ 1.0) excluded: 0\n",
      "ğŸ” No-change correct samples (FP=0, FN=0) excluded: 1823\n",
      "   Remaining samples with errors: 877/2700\n",
      "\n",
      "ğŸ“ˆ F1 Score Statistics (samples with errors):\n",
      "  Count: 877\n",
      "  Mean: 0.3754\n",
      "  Std: 0.4129\n",
      "  Min: 0.0000\n",
      "  Max: 0.9995\n",
      "  Median: 0.0000\n",
      "\n",
      "ğŸ“ˆ F1 Score Statistics (all samples):\n",
      "  Count: 2700\n",
      "  Mean: 0.1219\n",
      "\n",
      "ğŸ“Š Error Statistics (filtered samples):\n",
      "  Total FP (False Positives): 2113855\n",
      "  Total FN (False Negatives): 1487347\n",
      "  Average FP per sample: 2410.32\n",
      "  Average FN per sample: 1695.95\n",
      "  Total errors: 3601202\n",
      "\n",
      "ğŸ”´ Worst 10 samples (highest error rate):\n",
      "  1. test_0009_0001\n",
      "      F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "      FP=0, FN=55943, Total Errors=55943\n",
      "  2. test_0056_0020\n",
      "      F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "      FP=0, FN=31368, Total Errors=31368\n",
      "  3. test_0014_0017\n",
      "      F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "      FP=0, FN=25012, Total Errors=25012\n",
      "  4. test_0014_0018\n",
      "      F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "      FP=1296, FN=21042, Total Errors=22338\n",
      "  5. test_0022_0028\n",
      "      F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "      FP=0, FN=17141, Total Errors=17141\n",
      "  6. test_0008_0001\n",
      "      F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "      FP=0, FN=15832, Total Errors=15832\n",
      "  7. test_0005_0011\n",
      "      F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "      FP=1452, FN=14897, Total Errors=16349\n",
      "  8. test_0054_0015\n",
      "      F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "      FP=0, FN=14310, Total Errors=14310\n",
      "  9. test_0054_0014\n",
      "      F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "      FP=0, FN=13919, Total Errors=13919\n",
      "  10. test_0056_0034\n",
      "      F1=0.0000, Prec=0.0000, Rec=0.0000\n",
      "      FP=0, FN=11093, Total Errors=11093\n",
      "\n",
      "ğŸŸ¢ Best 10 samples (from samples with errors):\n",
      "  1. test_0011_0000\n",
      "      F1=0.9773, Prec=0.9690, Rec=0.9857\n",
      "      FP=1643, FN=743, Total Errors=2386\n",
      "  2. test_0027_0008\n",
      "      F1=0.9773, Prec=0.9556, Rec=1.0000\n",
      "      FP=2137, FN=0, Total Errors=2137\n",
      "  3. test_0029_0006\n",
      "      F1=0.9791, Prec=0.9664, Rec=0.9922\n",
      "      FP=888, FN=200, Total Errors=1088\n",
      "  4. test_0010_0000\n",
      "      F1=0.9833, Prec=0.9999, Rec=0.9673\n",
      "      FP=7, FN=2145, Total Errors=2152\n",
      "  5. test_0012_0001\n",
      "      F1=0.9863, Prec=0.9876, Rec=0.9849\n",
      "      FP=654, FN=798, Total Errors=1452\n",
      "  6. test_0023_0005\n",
      "      F1=0.9864, Prec=0.9763, Rec=0.9968\n",
      "      FP=1522, FN=203, Total Errors=1725\n",
      "  7. test_0011_0001\n",
      "      F1=0.9923, Prec=0.9850, Rec=0.9997\n",
      "      FP=889, FN=18, Total Errors=907\n",
      "  8. test_0028_0007\n",
      "      F1=0.9937, Prec=0.9879, Rec=0.9997\n",
      "      FP=693, FN=19, Total Errors=712\n",
      "  9. test_0027_0007\n",
      "      F1=0.9969, Prec=0.9993, Rec=0.9945\n",
      "      FP=46, FN=358, Total Errors=404\n",
      "  10. test_0028_0006\n",
      "      F1=0.9995, Prec=0.9990, Rec=1.0000\n",
      "      FP=65, FN=0, Total Errors=65\n",
      "\n",
      "ğŸ“Š Visualizing worst 400 and best 100 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Visualizing worst:  37%|â–ˆâ–ˆâ–ˆâ–‹      | 149/400 [02:13<03:37,  1.15it/s]"
     ]
    }
   ],
   "source": [
    "# F1 Score ê³„ì‚° í•¨ìˆ˜\n",
    "def calculate_f1_score(pred_mask, gt_mask, threshold=0.5):\n",
    "    \"\"\"ê°œë³„ ìƒ˜í”Œì˜ F1 Score ê³„ì‚°\"\"\"\n",
    "    pred_binary = (pred_mask > threshold).float()\n",
    "    gt_binary = (gt_mask > 0.5).float()\n",
    "    \n",
    "    # TP, FP, FN, TN ê³„ì‚°\n",
    "    tp = (pred_binary * gt_binary).sum().item()\n",
    "    fp = (pred_binary * (1 - gt_binary)).sum().item()\n",
    "    fn = ((1 - pred_binary) * gt_binary).sum().item()\n",
    "    tn = ((1 - pred_binary) * (1 - gt_binary)).sum().item()\n",
    "    \n",
    "    # F1 ê³„ì‚°\n",
    "    precision = tp / (tp + fp + 1e-7)\n",
    "    recall = tp / (tp + fn + 1e-7)\n",
    "    f1 = 2 * precision * recall / (precision + recall + 1e-7)\n",
    "    \n",
    "    return f1, precision, recall, tp, fp, fn, tn\n",
    "\n",
    "# F1 Score ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "sample_scores = []\n",
    "\n",
    "print(\"ğŸ“Š Calculating F1 scores for all samples...\")\n",
    "\n",
    "# ëª¨ë“  ìƒ˜í”Œì˜ F1 Score ê³„ì‚°\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm(test_loader, desc=\"Calculating F1\")):\n",
    "        try:\n",
    "            # ë°°ì¹˜ì—ì„œ ë°ì´í„° ì¶”ì¶œ\n",
    "            pre_img = batch['img1'].to(DEVICE)\n",
    "            post_img = batch['img2'].to(DEVICE)\n",
    "            gt_mask = batch['label'].to(DEVICE)\n",
    "            filename = batch['filename']\n",
    "            \n",
    "            # íŒŒì¼ëª… ì²˜ë¦¬\n",
    "            if isinstance(filename, (list, tuple)):\n",
    "                file_name = filename[0]\n",
    "            else:\n",
    "                file_name = filename\n",
    "            \n",
    "            # ì˜ˆì¸¡\n",
    "            pred_mask = model(pre_img, post_img)\n",
    "            \n",
    "            # Sigmoid ì ìš©\n",
    "            if pred_mask.max() > 1.0 or pred_mask.min() < 0:\n",
    "                pred_mask = torch.sigmoid(pred_mask)\n",
    "            \n",
    "            # F1 Score ê³„ì‚°\n",
    "            f1, precision, recall, tp, fp, fn, tn = calculate_f1_score(\n",
    "                pred_mask[0], gt_mask[0]\n",
    "            )\n",
    "            \n",
    "            # ê²°ê³¼ ì €ì¥\n",
    "            sample_scores.append({\n",
    "                'idx': idx,\n",
    "                'filename': Path(file_name).stem,\n",
    "                'f1': f1,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'tp': tp,\n",
    "                'fp': fp,\n",
    "                'fn': fn,\n",
    "                'tn': tn,\n",
    "                'batch': batch  # ë‚˜ì¤‘ì— ì‹œê°í™”í•  ë•Œ ì‚¬ìš©\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nâš ï¸ Error processing sample {idx}: {e}\")\n",
    "            continue\n",
    "\n",
    "print(f\"\\nâœ… F1 scores calculated for {len(sample_scores)} samples\")\n",
    "\n",
    "# í•„í„°ë§\n",
    "original_count = len(sample_scores)\n",
    "sample_scores_filtered = sample_scores.copy()\n",
    "\n",
    "# 1. F1 = 1.0ì¸ ì™„ë²½í•œ ìƒ˜í”Œ ì œì™¸\n",
    "if EXCLUDE_PERFECT:\n",
    "    before = len(sample_scores_filtered)\n",
    "    sample_scores_filtered = [s for s in sample_scores_filtered if s['f1'] < 0.9999]\n",
    "    excluded_perfect = before - len(sample_scores_filtered)\n",
    "    print(f\"\\nğŸ” Perfect samples (F1 â‰ˆ 1.0) excluded: {excluded_perfect}\")\n",
    "\n",
    "# 2. FP=0, FN=0ì¸ ìƒ˜í”Œ ì œì™¸ (ë³€í™” ì—†ëŠ” ì •ë‹µ ì¼€ì´ìŠ¤)\n",
    "if EXCLUDE_NO_CHANGE:\n",
    "    before = len(sample_scores_filtered)\n",
    "    sample_scores_filtered = [s for s in sample_scores_filtered if not (s['fp'] == 0 and s['fn'] == 0)]\n",
    "    excluded_no_change = before - len(sample_scores_filtered)\n",
    "    print(f\"ğŸ” No-change correct samples (FP=0, FN=0) excluded: {excluded_no_change}\")\n",
    "\n",
    "print(f\"   Remaining samples with errors: {len(sample_scores_filtered)}/{original_count}\")\n",
    "\n",
    "if len(sample_scores_filtered) == 0:\n",
    "    print(\"\\nâš ï¸ No samples with errors found! All predictions are perfect.\")\n",
    "    print(\"=\"*50)\n",
    "else:\n",
    "    # F1 Score ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ (ë‚®ì€ ìˆœì„œ)\n",
    "    # F1ì´ ê°™ìœ¼ë©´ FNì´ ë†’ì€ ìˆœì„œë¡œ (ë” ë§ì´ ë†“ì¹œ ê²ƒì„ ìš°ì„ )\n",
    "    sample_scores_sorted = sorted(sample_scores_filtered, key=lambda x: (x['f1'], -x['fn']))\n",
    "    \n",
    "    # í†µê³„ ì¶œë ¥\n",
    "    f1_values = [s['f1'] for s in sample_scores_filtered]\n",
    "    f1_values_all = [s['f1'] for s in sample_scores]\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ F1 Score Statistics (samples with errors):\")\n",
    "    print(f\"  Count: {len(f1_values)}\")\n",
    "    print(f\"  Mean: {np.mean(f1_values):.4f}\")\n",
    "    print(f\"  Std: {np.std(f1_values):.4f}\")\n",
    "    print(f\"  Min: {np.min(f1_values):.4f}\")\n",
    "    print(f\"  Max: {np.max(f1_values):.4f}\")\n",
    "    print(f\"  Median: {np.median(f1_values):.4f}\")\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ F1 Score Statistics (all samples):\")\n",
    "    print(f\"  Count: {len(f1_values_all)}\")\n",
    "    print(f\"  Mean: {np.mean(f1_values_all):.4f}\")\n",
    "    \n",
    "    # ì˜¤ë¥˜ í†µê³„\n",
    "    total_fp = sum([s['fp'] for s in sample_scores_filtered])\n",
    "    total_fn = sum([s['fn'] for s in sample_scores_filtered])\n",
    "    avg_fp = total_fp / len(sample_scores_filtered)\n",
    "    avg_fn = total_fn / len(sample_scores_filtered)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Error Statistics (filtered samples):\")\n",
    "    print(f\"  Total FP (False Positives): {int(total_fp)}\")\n",
    "    print(f\"  Total FN (False Negatives): {int(total_fn)}\")\n",
    "    print(f\"  Average FP per sample: {avg_fp:.2f}\")\n",
    "    print(f\"  Average FN per sample: {avg_fn:.2f}\")\n",
    "    print(f\"  Total errors: {int(total_fp + total_fn)}\")\n",
    "    \n",
    "    # Worst 10 ì¶œë ¥\n",
    "    print(f\"\\nğŸ”´ Worst 10 samples (highest error rate):\")\n",
    "    for i, sample in enumerate(sample_scores_sorted[:10]):\n",
    "        total_errors = int(sample['fp'] + sample['fn'])\n",
    "        print(f\"  {i+1}. {sample['filename']}\")\n",
    "        print(f\"      F1={sample['f1']:.4f}, Prec={sample['precision']:.4f}, Rec={sample['recall']:.4f}\")\n",
    "        print(f\"      FP={int(sample['fp'])}, FN={int(sample['fn'])}, Total Errors={total_errors}\")\n",
    "    \n",
    "    # Best 10 ì¶œë ¥ (ì—ëŸ¬ê°€ ìˆì§€ë§Œ ìƒëŒ€ì ìœ¼ë¡œ ì¢‹ì€ ì¼€ì´ìŠ¤)\n",
    "    print(f\"\\nğŸŸ¢ Best 10 samples (from samples with errors):\")\n",
    "    for i, sample in enumerate(sample_scores_sorted[-10:]):\n",
    "        total_errors = int(sample['fp'] + sample['fn'])\n",
    "        print(f\"  {i+1}. {sample['filename']}\")\n",
    "        print(f\"      F1={sample['f1']:.4f}, Prec={sample['precision']:.4f}, Rec={sample['recall']:.4f}\")\n",
    "        print(f\"      FP={int(sample['fp'])}, FN={int(sample['fn'])}, Total Errors={total_errors}\")\n",
    "    \n",
    "    # F1 ê¸°ë°˜ ì‹œê°í™” ì €ì¥ ë””ë ‰í† ë¦¬\n",
    "    f1_vis_dir = exp_dir / \"f1_visualization\"\n",
    "    f1_vis_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Visualizing worst {min(NUM_WORST_SAMPLES, len(sample_scores_sorted))} and best {min(NUM_BEST_SAMPLES, len(sample_scores_sorted))} samples...\")\n",
    "    \n",
    "    # Worst ìƒ˜í”Œ ì‹œê°í™”\n",
    "    worst_dir = f1_vis_dir / \"worst\"\n",
    "    worst_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    num_worst = min(NUM_WORST_SAMPLES, len(sample_scores_sorted))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for rank, sample_info in enumerate(tqdm(sample_scores_sorted[:num_worst], \n",
    "                                                 desc=\"Visualizing worst\")):\n",
    "            batch = sample_info['batch']\n",
    "            \n",
    "            pre_img = batch['img1'].to(DEVICE)\n",
    "            post_img = batch['img2'].to(DEVICE)\n",
    "            gt_mask = batch['label'].to(DEVICE)\n",
    "            \n",
    "            # ì˜ˆì¸¡\n",
    "            pred_mask = model(pre_img, post_img)\n",
    "            if pred_mask.max() > 1.0 or pred_mask.min() < 0:\n",
    "                pred_mask = torch.sigmoid(pred_mask)\n",
    "            \n",
    "            # íŒŒì¼ëª…ì— ë“±ìˆ˜ ì¶”ê°€\n",
    "            file_name = f\"{rank+1}.{sample_info['filename']}\"\n",
    "            \n",
    "            # ì‹œê°í™” ì €ì¥\n",
    "            save_visualization(\n",
    "                pre_img=pre_img[0],\n",
    "                post_img=post_img[0],\n",
    "                gt_mask=gt_mask[0],\n",
    "                pred_mask=pred_mask[0],\n",
    "                save_path=str(worst_dir),\n",
    "                file_name=file_name\n",
    "            )\n",
    "    \n",
    "    # Best ìƒ˜í”Œ ì‹œê°í™”\n",
    "    best_dir = f1_vis_dir / \"best\"\n",
    "    best_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    num_best = min(NUM_BEST_SAMPLES, len(sample_scores_sorted))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for rank, sample_info in enumerate(tqdm(sample_scores_sorted[-num_best:], \n",
    "                                                 desc=\"Visualizing best\")):\n",
    "            batch = sample_info['batch']\n",
    "            \n",
    "            pre_img = batch['img1'].to(DEVICE)\n",
    "            post_img = batch['img2'].to(DEVICE)\n",
    "            gt_mask = batch['label'].to(DEVICE)\n",
    "            \n",
    "            # ì˜ˆì¸¡\n",
    "            pred_mask = model(pre_img, post_img)\n",
    "            if pred_mask.max() > 1.0 or pred_mask.min() < 0:\n",
    "                pred_mask = torch.sigmoid(pred_mask)\n",
    "            \n",
    "            # íŒŒì¼ëª…ì— ë“±ìˆ˜ ì¶”ê°€\n",
    "            file_name = f\"{rank+1}.{sample_info['filename']}\"\n",
    "            \n",
    "            # ì‹œê°í™” ì €ì¥\n",
    "            save_visualization(\n",
    "                pre_img=pre_img[0],\n",
    "                post_img=post_img[0],\n",
    "                gt_mask=gt_mask[0],\n",
    "                pred_mask=pred_mask[0],\n",
    "                save_path=str(best_dir),\n",
    "                file_name=file_name\n",
    "            )\n",
    "    # Best ìƒ˜í”Œ ì‹œê°í™”\n",
    "    best_dir = f1_vis_dir / \"best\"\n",
    "    best_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    num_best = min(NUM_BEST_SAMPLES, len(sample_scores_sorted))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for rank, sample_info in enumerate(tqdm(sample_scores_sorted[-num_best:], \n",
    "                                                 desc=\"Visualizing best\")):\n",
    "            batch = sample_info['batch']\n",
    "            \n",
    "            pre_img = batch['img1'].to(DEVICE)\n",
    "            post_img = batch['img2'].to(DEVICE)\n",
    "            gt_mask = batch['label'].to(DEVICE)\n",
    "            \n",
    "            # ì˜ˆì¸¡\n",
    "            pred_mask = model(pre_img, post_img)\n",
    "            if pred_mask.max() > 1.0 or pred_mask.min() < 0:\n",
    "                pred_mask = torch.sigmoid(pred_mask)\n",
    "            \n",
    "            # ì›ë³¸ íŒŒì¼ëª… ì‚¬ìš©\n",
    "            file_name = sample_info['filename']\n",
    "            \n",
    "            # ì‹œê°í™” ì €ì¥\n",
    "            save_visualization(\n",
    "                pre_img=pre_img[0],\n",
    "                post_img=post_img[0],\n",
    "                gt_mask=gt_mask[0],\n",
    "                pred_mask=pred_mask[0],\n",
    "                save_path=str(best_dir),\n",
    "                file_name=file_name\n",
    "            )\n",
    "    \n",
    "    # F1 Score ë¶„í¬ íˆìŠ¤í† ê·¸ë¨ ì €ì¥\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # í•„í„°ë§ëœ ìƒ˜í”Œ (ì—ëŸ¬ê°€ ìˆëŠ” ìƒ˜í”Œë§Œ)\n",
    "    axes[0].hist(f1_values, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    axes[0].axvline(np.mean(f1_values), color='r', linestyle='--', linewidth=2, label=f'Mean: {np.mean(f1_values):.4f}')\n",
    "    axes[0].axvline(np.median(f1_values), color='g', linestyle='--', linewidth=2, label=f'Median: {np.median(f1_values):.4f}')\n",
    "    axes[0].set_xlabel('F1 Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title(f'F1 Score Distribution (Samples with Errors, n={len(f1_values)})')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # ì „ì²´ ìƒ˜í”Œ\n",
    "    axes[1].hist(f1_values_all, bins=50, edgecolor='black', alpha=0.7, color='lightcoral')\n",
    "    axes[1].axvline(np.mean(f1_values_all), color='r', linestyle='--', linewidth=2, label=f'Mean: {np.mean(f1_values_all):.4f}')\n",
    "    axes[1].axvline(np.median(f1_values_all), color='g', linestyle='--', linewidth=2, label=f'Median: {np.median(f1_values_all):.4f}')\n",
    "    axes[1].set_xlabel('F1 Score')\n",
    "    axes[1].set_ylabel('Frequency')\n",
    "    axes[1].set_title(f'F1 Score Distribution (All Samples, n={len(f1_values_all)})')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{TEST_MODEL} on {TEST_DATASET}', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f1_vis_dir / 'f1_distribution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # F1 Score ìƒì„¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥\n",
    "    score_summary = {\n",
    "        'model': TEST_MODEL,\n",
    "        'dataset': TEST_DATASET,\n",
    "        'total_samples': len(sample_scores),\n",
    "        'samples_with_errors': len(sample_scores_filtered),\n",
    "        'perfect_samples': len(sample_scores) - len(sample_scores_filtered) if EXCLUDE_PERFECT else 0,\n",
    "        'exclude_perfect': EXCLUDE_PERFECT,\n",
    "        'exclude_no_change': EXCLUDE_NO_CHANGE,\n",
    "        'error_statistics': {\n",
    "            'total_fp': int(total_fp),\n",
    "            'total_fn': int(total_fn),\n",
    "            'avg_fp_per_sample': float(avg_fp),\n",
    "            'avg_fn_per_sample': float(avg_fn),\n",
    "            'total_errors': int(total_fp + total_fn)\n",
    "        },\n",
    "        'statistics_filtered': {\n",
    "            'mean': float(np.mean(f1_values)),\n",
    "            'std': float(np.std(f1_values)),\n",
    "            'min': float(np.min(f1_values)),\n",
    "            'max': float(np.max(f1_values)),\n",
    "            'median': float(np.median(f1_values)),\n",
    "            'q1': float(np.percentile(f1_values, 25)),\n",
    "            'q3': float(np.percentile(f1_values, 75))\n",
    "        },\n",
    "        'statistics_all': {\n",
    "            'mean': float(np.mean(f1_values_all)),\n",
    "            'std': float(np.std(f1_values_all)),\n",
    "            'min': float(np.min(f1_values_all)),\n",
    "            'max': float(np.max(f1_values_all)),\n",
    "            'median': float(np.median(f1_values_all)),\n",
    "            'q1': float(np.percentile(f1_values_all, 25)),\n",
    "            'q3': float(np.percentile(f1_values_all, 75))\n",
    "        },\n",
    "        'worst_10': [\n",
    "            {\n",
    "                'rank': i+1,\n",
    "                'filename': s['filename'],\n",
    "                'f1': float(s['f1']),\n",
    "                'precision': float(s['precision']),\n",
    "                'recall': float(s['recall']),\n",
    "                'tp': int(s['tp']),\n",
    "                'fp': int(s['fp']),\n",
    "                'fn': int(s['fn']),\n",
    "                'tn': int(s['tn']),\n",
    "                'total_errors': int(s['fp'] + s['fn'])\n",
    "            }\n",
    "            for i, s in enumerate(sample_scores_sorted[:10])\n",
    "        ],\n",
    "        'best_10': [\n",
    "            {\n",
    "                'rank': i+1,\n",
    "                'filename': s['filename'],\n",
    "                'f1': float(s['f1']),\n",
    "                'precision': float(s['precision']),\n",
    "                'recall': float(s['recall']),\n",
    "                'tp': int(s['tp']),\n",
    "                'fp': int(s['fp']),\n",
    "                'fn': int(s['fn']),\n",
    "                'tn': int(s['tn']),\n",
    "                'total_errors': int(s['fp'] + s['fn'])\n",
    "            }\n",
    "            for i, s in enumerate(sample_scores_sorted[-10:])\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    with open(f1_vis_dir / 'f1_scores_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(score_summary, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"âœ… F1-based visualization completed!\")\n",
    "    print(f\"ğŸ“ Saved to: {f1_vis_dir}\")\n",
    "    print(f\"  - Worst {num_worst} samples: {worst_dir}\")\n",
    "    print(f\"  - Best {num_best} samples: {best_dir}\")\n",
    "    print(f\"  - F1 distribution plot: f1_distribution.png\")\n",
    "    print(f\"  - Summary JSON: f1_scores_summary.json\")\n",
    "    print(f\"\\nğŸ“Š Filtering Summary:\")\n",
    "    print(f\"  - Total samples: {original_count}\")\n",
    "    print(f\"  - Samples with errors: {len(sample_scores_filtered)} ({len(sample_scores_filtered)/original_count*100:.1f}%)\")\n",
    "    print(f\"  - Perfect samples excluded: {original_count - len(sample_scores_filtered)}\")\n",
    "    print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cd_efficient",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
