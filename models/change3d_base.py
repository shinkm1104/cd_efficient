"""
Change3D Base Implementation
ê·¹ë‹¨ì ìœ¼ë¡œ ìµœì†Œí™”ëœ Video Modeling for Change Detection
ì´ê²ƒë³´ë‹¤ ë” ì¤„ì´ë©´ Video Modeling íŒ¨ëŸ¬ë‹¤ì„ì´ ë¬´ë„ˆì§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class Change3DBase(nn.Module):
    """
    ìµœì†Œ Change3D ë„¤íŠ¸ì›Œí¬
    
    í•µì‹¬ ì•„ì´ë””ì–´ë§Œ:
    1. Perception Frame (í•™ìŠµ ê°€ëŠ¥í•œ "ê´€ì°°ì")
    2. 3D Convolution (ë¹„ë””ì˜¤ì²˜ëŸ¼ ì²˜ë¦¬)
    3. Time ì°¨ì›ì—ì„œ Perception Feature ì¶”ì¶œ
    
    A2Net Baseì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ë‹¨ìˆœí•¨!
    """
    
    def __init__(self, num_classes=1):
        super(Change3DBase, self).__init__()
        
        # ğŸ¯ í•µì‹¬ 1: Perception Frame
        # bi-temporal ì´ë¯¸ì§€ ì‚¬ì´ì— ë¼ì›Œë„£ì„ "ê´€ì°°ì"
        self.perception_frame = nn.Parameter(
            torch.randn(1, 3, 256, 256) * 0.02
        )
        
        # ğŸ¯ í•µì‹¬ 2: 3D Convolution (ë¹„ë””ì˜¤ ì¸ì½”ë”)
        # [I1, P, I2]ë¥¼ Time ì°¨ì›ìœ¼ë¡œ ì²˜ë¦¬
        # Time ì°¨ì›ë„ ê°™ì´ ì„ì–´ì•¼ ë³€í™” í•™ìŠµ ê°€ëŠ¥!
        
        # Stage 1: [B, 3, 3, H, W] -> [B, 32, 3, H/2, W/2]
        self.conv3d_1 = nn.Conv3d(
            3, 32, 
            kernel_size=(3, 3, 3),  # Time ì°¨ì›ë„ ì„ìŒ!
            stride=(1, 2, 2), 
            padding=(1, 1, 1)       # Time padding ì¶”ê°€
        )
        self.bn1 = nn.BatchNorm3d(32)
        
        # Stage 2: [B, 32, 3, H/2, W/2] -> [B, 64, 3, H/4, W/4]
        self.conv3d_2 = nn.Conv3d(
            32, 64,
            kernel_size=(3, 3, 3),  # Time ì°¨ì› ì„ìŒ
            stride=(1, 2, 2),
            padding=(1, 1, 1)
        )
        self.bn2 = nn.BatchNorm3d(64)
        
        # Stage 3: [B, 64, 3, H/4, W/4] -> [B, 128, 3, H/8, W/8]
        self.conv3d_3 = nn.Conv3d(
            64, 128,
            kernel_size=(3, 3, 3),  # Time ì°¨ì› ì„ìŒ
            stride=(1, 2, 2),
            padding=(1, 1, 1)
        )
        self.bn3 = nn.BatchNorm3d(128)
        
        # Stage 4: [B, 128, 3, H/8, W/8] -> [B, 256, 3, H/16, W/16]
        self.conv3d_4 = nn.Conv3d(
            128, 256,
            kernel_size=(3, 3, 3),  # Time ì°¨ì› ì„ìŒ
            stride=(1, 2, 2),
            padding=(1, 1, 1)
        )
        self.bn4 = nn.BatchNorm3d(256)
        
        # ğŸ¯ í•µì‹¬ 3: ë³€í™” íƒì§€ í—¤ë“œ (A2Netì²˜ëŸ¼ 1x1 Conv)
        self.head = nn.Conv2d(256, num_classes, kernel_size=1)
        
    def forward(self, t1, t2):
        """
        Args:
            t1: Time 1 ì´ë¯¸ì§€ [B, 3, H, W]
            t2: Time 2 ì´ë¯¸ì§€ [B, 3, H, W]
        Returns:
            change_map: [B, num_classes, H, W]
        """
        B, C, H, W = t1.shape
        
        # 1. Perception Frame í™•ì¥
        P = self.perception_frame.expand(B, -1, -1, -1)  # [B, 3, H, W]
        
        # 2. ë¹„ë””ì˜¤ êµ¬ì„±: [I1, P, I2] - Time ì°¨ì›ìœ¼ë¡œ ìŒ“ê¸°
        # ë…¼ë¬¸ì˜ í•µì‹¬ ì•„ì´ë””ì–´!
        video = torch.stack([t1, P, t2], dim=2)  # [B, 3, 3, H, W]
        #                                             â†‘ Time=3
        
        # 3. 3D Convolutionìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
        x = F.relu(self.bn1(self.conv3d_1(video)))  # [B, 32, 3, H/2, W/2]
        x = F.relu(self.bn2(self.conv3d_2(x)))      # [B, 64, 3, H/4, W/4]
        x = F.relu(self.bn3(self.conv3d_3(x)))      # [B, 128, 3, H/8, W/8]
        x = F.relu(self.bn4(self.conv3d_4(x)))      # [B, 256, 3, H/16, W/16]
        
        # 4. Perception Feature ì¶”ì¶œ
        # Time ì°¨ì›ì—ì„œ ì¤‘ê°„(index=1) = Perception Frameì˜ íŠ¹ì§•
        # ì´ê²Œ Change3Dì˜ í•µì‹¬ íŠ¸ë¦­!
        perception_feat = x[:, :, 1, :, :]  # [B, 256, H/16, W/16]
        
        # 5. ë³€í™” ë§µ ìƒì„± (A2Netì²˜ëŸ¼ 1x1 Conv)
        change = self.head(perception_feat)  # [B, 1, H/16, W/16]
        
        # 6. ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
        change_map = F.interpolate(
            change, 
            size=(H, W), 
            mode='bilinear', 
            align_corners=False
        )
        
        return change_map


if __name__ == "__main__":
    print("="*60)
    print("Change3D Base - Minimal Video Modeling for Change Detection")
    print("="*60)
    
    # ëª¨ë¸ ìƒì„±
    model = Change3DBase(num_classes=1)
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    t1 = torch.randn(2, 3, 256, 256)
    t2 = torch.randn(2, 3, 256, 256)
    
    # Forward
    output = model(t1, t2)
    
    print(f"\nInput shapes:")
    print(f"  t1: {t1.shape}")
    print(f"  t2: {t2.shape}")
    print(f"\nOutput shape:")
    print(f"  change_map: {output.shape}")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nModel Parameters:")
    print(f"  Total: {total_params:,}")
    print(f"  Trainable: {trainable_params:,}")
    print(f"  Size: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)")
    
    # A2Netê³¼ ë¹„êµ
    print(f"\nComparison with A2Net Base:")
    print(f"  A2Net Base: ~3.5M params")
    print(f"  Change3D Base: {total_params/1e6:.1f}M params")
    print(f"  Ratio: {total_params/3.5e6:.1f}x")
    
    print("\n" + "="*60)
    print("âœ“ Change3D Base implementation complete!")
    print("  - Only ~130 lines (vs 200+ before)")
    print("  - No separate Backbone/Decoder classes")
    print("  - Just 4 Conv3D layers + 1 head")
    print("  - Similar complexity to A2Net Base")
    print("="*60)