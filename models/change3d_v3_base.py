"""
Change3D Base Implementation
ê·¹ë‹¨ì ìœ¼ë¡œ ìµœì†Œí™”ëœ Video Modeling for Change Detection
ì´ê²ƒë³´ë‹¤ ë” ì¤„ì´ë©´ Video Modeling íŒ¨ëŸ¬ë‹¤ì„ì´ ë¬´ë„ˆì§
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import resnet18


class Change3D_v2Base(nn.Module):
    """
    ìµœì†Œ Change3D ë„¤íŠ¸ì›Œí¬
    
    í•µì‹¬ ì•„ì´ë””ì–´ë§Œ:
    1. Perception Frame (í•™ìŠµ ê°€ëŠ¥í•œ "ê´€ì°°ì")
    2. Pretrained 2D Backbone (Spatial íŠ¹ì§• ì¶”ì¶œ)
    3. 3D Conv 1ê°œ (Time ì°¨ì›ë§Œ ì„ê¸°)
    4. Perception Feature ì¶”ì¶œ
    
    A2Net Baseì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ì˜ ë‹¨ìˆœí•¨!
    """
    
    def __init__(self, num_classes=1):
        super(Change3D_v2Base, self).__init__()
        
        # ğŸ¯ í•µì‹¬ 1: Perception Frame
        # bi-temporal ì´ë¯¸ì§€ ì‚¬ì´ì— ë¼ì›Œë„£ì„ "ê´€ì°°ì"
        self.perception_frame = nn.Parameter(
            torch.randn(1, 3, 256, 256) * 0.02
        )
        
        # ğŸ¯ í•µì‹¬ 2: 2D Pretrained Backbone
        # Spatial íŠ¹ì§•ì€ ì´ë¯¸ í•™ìŠµëœ ResNetì´ ì¶”ì¶œ!
        backbone = resnet18(pretrained=True)
        self.encoder = nn.Sequential(
            backbone.conv1,
            backbone.bn1,
            backbone.relu,
            backbone.maxpool,
            backbone.layer1,  # 64 channels
            backbone.layer2   # 128 channels, H/8
        )
        
        # ğŸ¯ í•µì‹¬ 3: 3D Conv (Time ì°¨ì›ë§Œ ì„ê¸°)
        # [I1, P, I2] ì‚¬ì´ì˜ ì‹œê°„ì  ê´€ê³„ë§Œ í•™ìŠµ
        self.conv3d = nn.Conv3d(
            128, 256,
            kernel_size=(3, 3, 3),  # Timeë„ ì„ìŒ
            stride=(1, 2, 2),       # Spatialë§Œ stride
            padding=(1, 1, 1)
        )
        self.bn3d = nn.BatchNorm3d(256)
        
        # ğŸ¯ í•µì‹¬ 4: ë³€í™” íƒì§€ í—¤ë“œ
        self.head = nn.Conv2d(256, num_classes, kernel_size=1)
        
    def forward(self, t1, t2):
        """
        Args:
            t1: Time 1 ì´ë¯¸ì§€ [B, 3, H, W]
            t2: Time 2 ì´ë¯¸ì§€ [B, 3, H, W]
        Returns:
            change_map: [B, num_classes, H, W]
        """
        B, C, H, W = t1.shape
        
        # 1. Perception Frame í™•ì¥
        P = self.perception_frame.expand(B, -1, -1, -1)  # [B, 3, H, W]
        
        # 2. [I1, P, I2] ê°ê° 2D Backboneìœ¼ë¡œ íŠ¹ì§• ì¶”ì¶œ
        # 3ê°œ í”„ë ˆì„ì„ ë°°ì¹˜ë¡œ í•©ì¹¨
        frames = torch.stack([t1, P, t2], dim=1)  # [B, 3, 3, H, W]
        frames = frames.view(B * 3, C, H, W)      # [B*3, 3, H, W]
        
        # 2D Backboneìœ¼ë¡œ Spatial íŠ¹ì§• ì¶”ì¶œ (Pretrained!)
        feat = self.encoder(frames)  # [B*3, 128, H/8, W/8]
        
        # 3. 3Dë¡œ reshape
        _, C_feat, H_feat, W_feat = feat.shape
        feat = feat.view(B, 3, C_feat, H_feat, W_feat)  # [B, 3, 128, H/8, W/8]
        
        # Timeì„ ì±„ë„ ì°¨ì› ì•ìœ¼ë¡œ
        feat = feat.permute(0, 2, 1, 3, 4)  # [B, 128, 3, H/8, W/8]
        
        # 4. 3D Convë¡œ Time ì°¨ì› ì„ê¸°
        # [I1, P, I2] ì‚¬ì´ì˜ ì‹œê°„ì  ê´€ê³„ í•™ìŠµ!
        feat = F.relu(self.bn3d(self.conv3d(feat)))  # [B, 256, 3, H/16, W/16]
        
        # 5. Perception Feature ì¶”ì¶œ
        # Time ì°¨ì›ì—ì„œ ì¤‘ê°„(index=1) = Perception Frameì˜ íŠ¹ì§•
        perception_feat = feat[:, :, 1, :, :]  # [B, 256, H/16, W/16]
        
        # 6. ë³€í™” ë§µ ìƒì„±
        change = self.head(perception_feat)  # [B, 1, H/16, W/16]
        
        # 7. ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
        change_map = F.interpolate(
            change,
            size=(H, W),
            mode='bilinear',
            align_corners=False
        )
        
        return change_map


if __name__ == "__main__":
    print("="*60)
    print("Change3D Base - Minimal Video Modeling for Change Detection")
    print("="*60)
    
    # ëª¨ë¸ ìƒì„±
    model = Change3DBase(num_classes=1)
    
    # í…ŒìŠ¤íŠ¸ ì…ë ¥
    t1 = torch.randn(2, 3, 256, 256)
    t2 = torch.randn(2, 3, 256, 256)
    
    # Forward
    output = model(t1, t2)
    
    print(f"\nInput shapes:")
    print(f"  t1: {t1.shape}")
    print(f"  t2: {t2.shape}")
    print(f"\nOutput shape:")
    print(f"  change_map: {output.shape}")
    
    # íŒŒë¼ë¯¸í„° ìˆ˜
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    print(f"\nModel Parameters:")
    print(f"  Total: {total_params:,}")
    print(f"  Trainable: {trainable_params:,}")
    print(f"  Size: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)")
    
    # A2Netê³¼ ë¹„êµ
    print(f"\nComparison with A2Net Base:")
    print(f"  A2Net Base: ~3.5M params")
    print(f"  Change3D Base: {total_params/1e6:.1f}M params")
    print(f"  Ratio: {total_params/3.5e6:.1f}x")
    
    print("\n" + "="*60)
    print("âœ“ Change3D Base implementation complete!")
    print("  - Only ~100 lines (vs 200+ before)")
    print("  - Pretrained ResNet18 (Spatial)")
    print("  - Just 1 Conv3D layer (Time only)")
    print("  - Similar complexity to A2Net Base")
    print("="*60)